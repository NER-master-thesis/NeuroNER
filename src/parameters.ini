[mode]
# At least one of use_pretrained_model and train_model must be set to True.

predict_text = input_demo.txt

train_model = True
#dataset_predict = ../../../wikiner_dataset/aij-wikiner-de-wp2-simplified
#dataset_predict = ../../../new_dataset/als/wp3/combined_processed_wp3_1.0.txt
#dataset_predict = ../../../new_dataset/als/wp3/combined_wp3_1.0.txt
#dataset_predict = ../../../ner/nerc-conll2003/deu.testb


use_pretrained_model = False
#pretrained_model_name = DE_on_alsP
#pretrained_model_checkpoint_filepath =  ../output/eng-simplified_2017-07-17_13-57-31-862607/model/model_00016.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_aij-wikiner-de-wp2-simplified_2017-08-09_13-34-58-801988/model/model_00004.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_deu_2017-08-10_18-45-42-177588/model/model_00014.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_combined_wp3_0.01_2017-08-08_04-00-24-97983/model/model_00016.ckpt

#EN
#pretrained_model_checkpoint_filepath =  ../output/en_combined_wp3_0.1_2017-08-14_23-28-52-854871/model/model_001.009.ckpt

#best CoNLL_EN
#pretrained_model_checkpoint_filepath =  ../output/en_eng_2017-08-14_15-28-23-794939/model/model_00006.ckpt
#dataset_predict = ../../../wikiner_dataset/aij-wikiner-en-wp2-simplified.test
#dataset_predict = ../../../new_dataset/en/wp3/combined_wp3_0.01.test
#pretrained_model_name = conll_EN
#language = en
#pretrained_model_checkpoint_filepath =  ../output/en_combined_wp3_0.005.txt_2017-08-10_17-07-38-585597/model/model_00016.ckpt
#pretrained_model_checkpoint_filepath =  ../output/en_eng_2017-08-14_15-28-23-794939/model/model_00006.ckpt
#pretrained_model_checkpoint_filepath =  ../output/en_aij-wikiner-en-wp2-simplified_2017-08-14_12-22-13-676681/model/model_00005.ckpt

#dataset_predict = ../../../wikiner_dataset/wikigold.conll.txt
#dataset_predict = ../../../wikiner_dataset/aij-wikiner-en-wp2-simplified.test
#dataset_predict = ../../../ner/nerc-conll2003/eng.testb
#pretrained_model_name = en_en
#language = en#DE
#language = de
#pretrained_model_checkpoint_filepath =  ../output/de_deu_2017-08-10_18-45-42-177588/model/model_00014.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_aij-wikiner-de-wp2-simplified_2017-08-14_15-35-59-298959/model/model_00001.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_combined_wp3_0.2_2017-08-15_08-24-48-871337/model/model_001.007.ckpt
#pretrained_model_name = de_alsP

#dataset_predict = ../../../wikiner_dataset/aij-wikiner-de-wp2-simplified.test
#dataset_predict = ../../../new_dataset/als/wp3/combined_processed_wp3_1.0.txt

#language = de
#ALS_proc
#pretrained_model_checkpoint_filepath =  ../output/als_proc_combined_processed_wp3_1.0_2017-08-10_20-30-20-695438/model/model_00006.ckpt
#pretrained_model_checkpoint_filepath =  ../output/de_combined_processed_wp3_1.0_2017-08-14_23-07-45-85927/model/model_00008.ckpt
#ALS
#pretrained_model_checkpoint_filepath =  ../output/als_combined_wp3_1.0_2017-08-10_20-29-36-810618/model/model_00002.ckpt
#pretrained_model_name = alsProc_wikiner
#dataset_predict = ../../../wikiner_dataset/aij-wikiner-de-wp2-simplified.test
[dataset]
#language = de
#dataset_train = ../../../new_dataset/de/wp3/combined_wp3_0.2.train
#dataset_train = ../../../new_dataset/de/wp3/combined_wp3_0.3.train
#dataset_train = ../../../wikiner_dataset/aij-wikiner-de-wp2-simplified.train
#dataset_valid = ../../../ner/nerc-conll2003/deu.testa
#dataset_test = ../../../ner/nerc-conll2003/deu.testb
#experiment_name = de_small

#language = it
#dataset_train = ../../../new_dataset/it/wp3/combined_wp3_0.3.train
#dataset_valid = ../../../new_dataset/it/wp3/combined_wp3_0.3.valid
#dataset_test = ../../../new_dataset/it/wp3/combined_wp3_0.3.test

#language = fr
#dataset_train = ../../../new_dataset/fr/wp3/combined_wp3_0.3.train
#dataset_valid = ../../../new_dataset/fr/wp3/combined_wp3_0.3.valid
#dataset_test = ../../../new_dataset/fr/wp3/combined_wp3_0.3.test

#als = ../../../new_dataset/als/wp3/combined_wp3_1.0

#language = de

lb = ../../../new_dataset/lb/wp3/combined_wp3_1.0
nds = ../../../new_dataset/nds/wp3/combined_wp3_1.0
ksh = ../../../new_dataset/ksh/wp3/combined_wp3_1.0
pfl = ../../../new_dataset/pfl/wp3/combined_wp3_1.0
pdc = ../../../new_dataset/pdc/wp3/combined_wp3_1.0

#dataset_train = ../../../new_dataset/als/wp3/combined_wp3_1.0.train
#dataset_valid = ../../../new_dataset/als/wp3/combined_wp3_1.0.valid
#dataset_test = ../../../new_dataset/als/wp3/combined_wp3_1.0.test

language = it
pms = ../../../new_dataset/pms/wp3/combined_wp3_1.0
lmo = ../../../new_dataset/lmo/wp3/combined_wp3_1.0
scn = ../../../new_dataset/scn/wp3/combined_wp3_1.0
vec = ../../../new_dataset/vec/wp3/combined_wp3_1.0
nap = ../../../new_dataset/nap/wp3/combined_wp3_1.0
sc = ../../../new_dataset/sc/wp3/combined_wp3_1.0
co = ../../../new_dataset/co/wp3/combined_wp3_1.0
rm = ../../../new_dataset/rm/wp3/combined_wp3_1.0
lij = ../../../new_dataset/lij/wp3/combined_wp3_1.0
fur = ../../../new_dataset/fur/wp3/combined_wp3_1.0

#data_to_use = 10000
#language = en
#dataset_train = ../../../new_dataset/en/wp3/combined_wp3_1.0.train
#dataset_train = ../../../wikiner_dataset/aij-wikiner-en-wp2-simplified.train
#dataset_train = ../../../ner/nerc-conll2003/eng.train
#dataset_valid = ../../../ner/nerc-conll2003/eng.testa
#dataset_test = ../../../ner/nerc-conll2003/eng.testb

#language = de
#cross_validation = 5
#dataset_train = ../../../new_dataset/als/wp3/wikipedia_dataset_1.0/combined_processed_wp3_1.0.txt
#dataset_valid = ../../../ner/nerc-conll2003/deu.testa
#dataset_test = ../../../ner/nerc-conll2003/deu.testb

# main_evaluation_mode should be either 'conll', 'bio', 'token', or 'binary'. ('conll' is entity-based)
# It determines which metric to use for early stopping, displaying during training, and plotting F1-score vs. epoch.
main_evaluation_mode = conll

#---------------------------------------------------------------------------------------------------------------------#
# The parameters below are for advanced users. Their default values should yield good performance in most cases.      #
#---------------------------------------------------------------------------------------------------------------------#

[ann]
use_crf = True
gru_neuron = False

use_character_lstm = True
character_embedding_dimension = 25
character_lstm_hidden_state_dimension = 25
character_hidden_layer = 1

# In order to use random initialization instead, set token_pretrained_embedding_filepath to empty string, as below:
# token_pretrained_embedding_filepath =
use_pretrained_embeddings = True
embedding_path = ../../../word_embeddings/
embedding_type = fasttext
embedding_dimension = 300
token_lstm_hidden_state_dimension = 100
token_hidden_layer = 1


[training]
patience = 5
maximum_number_of_epochs = 20
batch_size = 64
# optimizer should be either 'sgd', 'adam', or 'adadelta'
optimizer = adam
learning_rate = 0.0004
# gradients will be clipped above |gradient_clipping_value| and below -|gradient_clipping_value|, if gradient_clipping_value is non-zero 
# (set to 0 to disable gradient clipping)
gradient_clipping_value = 5.0

# dropout_rate should be between 0 and 1
dropout_rate = 0.5

# Upper bound on the number of CPU threads NeuroNER will use 
number_of_cpu_threads = 12

# Upper bound on the number of GPU NeuroNER will use 
# If number_of_gpus > 0, you need to have installed tensorflow-gpu
number_of_gpus = 1

[advanced]

# tagging_format should be either 'bioes' or 'bio'
tagging_format = bio

# If remap_unknown_tokens is set to True, map to UNK any token that hasn't been seen in neither the training set nor the pre-trained token embeddings.
remap_unknown_tokens_to_unk = True

# If load_only_pretrained_token_embeddings is set to True, then token embeddings will only be loaded if it exists in token_pretrained_embedding_filepath 
# or in pretrained_model_checkpoint_filepath, even for the training set.
load_only_pretrained_token_embeddings = False

# If check_for_lowercase is set to True, the lowercased version of each token will also be checked when loading the pretrained embeddings.
# For example, if the token 'Boston' does not exist in the pretrained embeddings, then it is mapped to the embedding of its lowercased version 'boston',
# if it exists among the pretrained embeddings.
check_for_lowercase = True

# If check_for_digits_replaced_with_zeros is set to True, each token with digits replaced with zeros will also be checked when loading pretrained embeddings.
# For example, if the token '123-456-7890' does not exist in the pretrained embeddings, then it is mapped to the embedding of '000-000-0000',
# if it exists among the pretrained embeddings.
# If both check_for_lowercase and check_for_digits_replaced_with_zeros are set to True, then the lowercased version is checked before the digit-zeroed version.
check_for_digits_replaced_with_zeros = True

# If freeze_token_embeddings is set to True, token embedding will remain frozen (not be trained).
freeze_token_embeddings = True

# If debug is set to True, only 200 lines will be loaded for each split of the dataset.
debug = False
verbose = False

# plot_format specifies the format of the plots generated by NeuroNER. It should be either 'png' or 'pdf'.
plot_format = pdf

# specify which layers to reload from the pretrained model
reload_character_embeddings = True
reload_character_lstm = True
reload_token_embeddings = True
reload_token_lstm = True
reload_feedforward = True
reload_crf = True
