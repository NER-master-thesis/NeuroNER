[mode]
# At least one of use_pretrained_model and train_model must be set to True.

predict_text = input_demo.txt

train_model = True
use_pretrained_model = False

[dataset]
language = de
dataset_train = ../../../new_dataset/de/wp3/combined_wp3_0.3.train
#dataset_train = ../../../wikiner_dataset/aij-wikiner-de-wp2-simplified.train
dataset_valid = ../../../ner/nerc-conll2003/deu.testa
dataset_test = ../../../ner/nerc-conll2003/deu.testb
#experiment_name = de_small

#language = it
#dataset_train = ../../../new_dataset/it/wp3/combined_wp3_0.3.train
#dataset_valid = ../../../new_dataset/it/wp3/combined_wp3_0.3.valid
#dataset_test = ../../../new_dataset/it/wp3/combined_wp3_0.3.test

#language = fr
#dataset_train = ../../../new_dataset/fr/wp3/combined_wp3_0.3.train
#dataset_valid = ../../../new_dataset/fr/wp3/combined_wp3_0.3.valid
#dataset_test = ../../../new_dataset/fr/wp3/combined_wp3_0.3.test

#als = ../../../new_dataset/als/wp3/combined_wp3_1.0

#language = de

;lb = ../../../new_dataset/lb/wp3/combined_wp3_1.0
;nds = ../../../new_dataset/nds/wp3/combined_wp3_1.0
;ksh = ../../../new_dataset/ksh/wp3/combined_wp3_1.0
;pfl = ../../../new_dataset/pfl/wp3/combined_wp3_1.0
;pdc = ../../../new_dataset/pdc/wp3/combined_wp3_1.0
;
;#dataset_train = ../../../new_dataset/als/wp3/combined_wp3_1.0.train
;#dataset_valid = ../../../new_dataset/als/wp3/combined_wp3_1.0.valid
;#dataset_test = ../../../new_dataset/als/wp3/combined_wp3_1.0.test
;
;language = it
;pms = ../../../new_dataset/pms/wp3/combined_wp3_1.0
;lmo = ../../../new_dataset/lmo/wp3/combined_wp3_1.0
;scn = ../../../new_dataset/scn/wp3/combined_wp3_1.0
;vec = ../../../new_dataset/vec/wp3/combined_wp3_1.0
;nap = ../../../new_dataset/nap/wp3/combined_wp3_1.0
;sc = ../../../new_dataset/sc/wp3/combined_wp3_1.0
;co = ../../../new_dataset/co/wp3/combined_wp3_1.0
;rm = ../../../new_dataset/rm/wp3/combined_wp3_1.0
;lij = ../../../new_dataset/lij/wp3/combined_wp3_1.0
;fur = ../../../new_dataset/fur/wp3/combined_wp3_1.0

#data_to_use = 10000

#language = en
#dataset_train = ../../../new_dataset/en/wp3/combined_wp3_1.0.train
#dataset_train = ../../../wikiner_dataset/aij-wikiner-en-wp2-simplified.train
#dataset_train = ../../../ner/nerc-conll2003/eng.train
#dataset_valid = ../../../ner/nerc-conll2003/eng.testa
#dataset_test = ../../../ner/nerc-conll2003/eng.testb

#language = de
#cross_validation = 5
#dataset_train = ../../../new_dataset/als/wp3/wikipedia_dataset_1.0/combined_processed_wp3_1.0.txt
#dataset_valid = ../../../ner/nerc-conll2003/deu.testa
#dataset_test = ../../../ner/nerc-conll2003/deu.testb

# main_evaluation_mode should be either 'conll', 'bio', 'token', or 'binary'. ('conll' is entity-based)
# It determines which metric to use for early stopping, displaying during training, and plotting F1-score vs. epoch.
main_evaluation_mode = conll

#---------------------------------------------------------------------------------------------------------------------#
# The parameters below are for advanced users. Their default values should yield good performance in most cases.      #
#---------------------------------------------------------------------------------------------------------------------#

[ann]
use_crf = True
gru_neuron = False

use_character_lstm = True
character_embedding_dimension = 25
character_lstm_hidden_state_dimension = 25
character_hidden_layer = 1

# In order to use random initialization instead, set token_pretrained_embedding_filepath to empty string, as below:
# token_pretrained_embedding_filepath =
use_pretrained_embeddings = True
embedding_path = ../../../word_embeddings/
embedding_type = fasttext_noOOV
embedding_dimension = 300
token_lstm_hidden_state_dimension = 100
token_hidden_layer = 1


[training]
patience = 5
maximum_number_of_epochs = 40
batch_size = 32
# optimizer should be either 'sgd', 'adam', or 'adadelta'
optimizer = adam
learning_rate = 0.0004
# gradients will be clipped above |gradient_clipping_value| and below -|gradient_clipping_value|, if gradient_clipping_value is non-zero 
# (set to 0 to disable gradient clipping)
gradient_clipping_value = 5.0

# dropout_rate should be between 0 and 1
dropout_rate = 0.5

# Upper bound on the number of CPU threads NeuroNER will use 
number_of_cpu_threads = 12

# Upper bound on the number of GPU NeuroNER will use 
# If number_of_gpus > 0, you need to have installed tensorflow-gpu
number_of_gpus = 1

[advanced]

# tagging_format should be either 'bioes' or 'bio'
tagging_format = bio

# If remap_unknown_tokens is set to True, map to UNK any token that hasn't been seen in neither the training set nor the pre-trained token embeddings.
remap_unknown_tokens_to_unk = True

# If load_only_pretrained_token_embeddings is set to True, then token embeddings will only be loaded if it exists in token_pretrained_embedding_filepath 
# or in pretrained_model_checkpoint_filepath, even for the training set.
load_only_pretrained_token_embeddings = False

# If check_for_lowercase is set to True, the lowercased version of each token will also be checked when loading the pretrained embeddings.
# For example, if the token 'Boston' does not exist in the pretrained embeddings, then it is mapped to the embedding of its lowercased version 'boston',
# if it exists among the pretrained embeddings.
check_for_lowercase = True

# If check_for_digits_replaced_with_zeros is set to True, each token with digits replaced with zeros will also be checked when loading pretrained embeddings.
# For example, if the token '123-456-7890' does not exist in the pretrained embeddings, then it is mapped to the embedding of '000-000-0000',
# if it exists among the pretrained embeddings.
# If both check_for_lowercase and check_for_digits_replaced_with_zeros are set to True, then the lowercased version is checked before the digit-zeroed version.
check_for_digits_replaced_with_zeros = True

# If freeze_token_embeddings is set to True, token embedding will remain frozen (not be trained).
freeze_token_embeddings = True

# If debug is set to True, only 200 lines will be loaded for each split of the dataset.
debug = False
verbose = False

# plot_format specifies the format of the plots generated by NeuroNER. It should be either 'png' or 'pdf'.
plot_format = pdf

# specify which layers to reload from the pretrained model
reload_character_embeddings = True
reload_character_lstm = True
reload_token_embeddings = True
reload_token_lstm = True
reload_feedforward = True
reload_crf = True
